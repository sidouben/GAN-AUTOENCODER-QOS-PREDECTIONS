{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GANS-QOS.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU0BdH_v20sP"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam ,RMSprop, SGD ,Adadelta ,Adagrad, Adamax\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GAN():\n",
        "    def __init__(self):\n",
        "        self.rows = 1\n",
        "        self.cols = 5825\n",
        "        self.channels = 1\n",
        "        self.qos_shape = (self.rows, self.cols)\n",
        "        self.latent_dim = 100\n",
        "        self.resultat = pd.DataFrame(columns = ['EPOCHS','RMSE_AUTOENCODERS'])\n",
        "\n",
        "        optimizer = Adam(0.001, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # The generator takes noise as input and generates vectors\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        qos = self.generator(z)\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # The discriminator takes generated vectors as input and determines validity\n",
        "        validity = self.discriminator(qos)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model(z, validity)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "\n",
        "    def build_generator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(256, input_dim=self.latent_dim))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.99))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.99))\n",
        "        model.add(Dense(1024))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(BatchNormalization(momentum=0.99))\n",
        "        model.add(Dense(np.prod(self.qos_shape), activation='relu'))\n",
        "        model.add(Reshape(self.qos_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        qos = model(noise)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def build_discriminator(self):\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Flatten(input_shape=self.qos_shape))\n",
        "        model.add(Dense(512))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(256))\n",
        "        model.add(LeakyReLU(alpha=0.2))\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        qos = Input(shape=self.qos_shape)\n",
        "        validity = model(qos)\n",
        "\n",
        "        return Model(qos, validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=273, sample_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        rtMatrix = pd.read_csv('./train.txt', delimiter=\"\\t\", header=None)\n",
        "        \n",
        "        # Rescale -1 to 1\n",
        "        X_train = np.expand_dims(rtMatrix, axis=1)\n",
        "        X_train = X_train / 20.\n",
        "        \n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random batch of vectors\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            v_qos = X_train\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Generate a batch of new vectors\n",
        "            gen_v_qos = self.generator.predict(noise)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(v_qos, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_v_qos, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch(noise, valid)\n",
        "            \n",
        "            if epoch % sample_interval == 0:\n",
        "                # three different tests\n",
        "\n",
        "                #one=self.mean_all(epoch,d_loss[0], 100*d_loss[1], g_loss)\n",
        "                #two=self.mean_every(epoch,d_loss[0], 100*d_loss[1], g_loss)\n",
        "                three=self.test_autoencoder(epoch)\n",
        "                self.resultat = self.resultat.append({'EPOCHS' : epoch, 'RMSE_AUTOENCODERS' : three}, ignore_index=True)\n",
        "            if epoch == epochs-1:\n",
        "                #Result \n",
        "\n",
        "                print(self.resultat)\n",
        "                self.resultat.to_csv('resultat.txt', index=False,sep='\\t',float_format='%.3f')\n",
        "    def root_mean_squared_error(self,y_true, y_pred):\n",
        "        K = tf.keras.backend\n",
        "        return K.sqrt(K.mean(K.square(y_pred - y_true)))     \n",
        "    def mean_all(self, epoch,d_loss, a_loss, g_loss):\n",
        "        test = pd.read_csv('./test.txt', delimiter=\"\\t\", header=None)\n",
        "        t_mean=test.mean()\n",
        "        noise = np.random.normal(0, 1, (66, self.latent_dim))\n",
        "        gen_qos = self.generator.predict(noise)\n",
        "        # Rescale \n",
        "        gen_qos = 20 * gen_qos\n",
        "        reduire=np.squeeze(gen_qos, axis=1)\n",
        "        df = pd.DataFrame(data=reduire)\n",
        "        mean = df.mean()\n",
        "        mse = self.root_mean_squared_error(t_mean, mean)\n",
        "        mse = mse.numpy()\n",
        "\n",
        "        print(epoch ,' rmse entre la moyenne de tout les vecteurs original et les vecteurs generer ' ,mse,'' )\n",
        "\n",
        "        return  mse\n",
        "\n",
        "    def mean_every(self, epoch,d_loss, a_loss, g_loss):\n",
        "        test = pd.read_csv('./test.txt', delimiter=\"\\t\", header=None) \n",
        "        t_mean=test.mean()\n",
        "        mean_final=0\n",
        "\n",
        "        #66 users\n",
        "\n",
        "        for i in range(66):\n",
        "            rt=test.iloc[i]\n",
        "            fin=np.expand_dims(rt, axis=0)\n",
        "            \n",
        "            noise = np.random.normal(0, 1, (1, self.latent_dim))\n",
        "            gen_qos = self.generator.predict(noise)\n",
        "            # Rescale \n",
        "            gen_qos = 20 * gen_qos\n",
        "            reduire=np.squeeze(gen_qos, axis=1)\n",
        "            df = pd.DataFrame(data=reduire)\n",
        "            mean = df.mean()\n",
        "\n",
        "            mse = self.root_mean_squared_error(fin, reduire)\n",
        "            mse = mse.numpy()\n",
        "            mean_final = mean_final + mse\n",
        "        result = mean_final/66\n",
        "\n",
        "        print(epoch ,' la moyenne de tout les rmse entre chaque vectueur original et generer ' ,result,' ' )\n",
        "        return result\n",
        "    def test_autoencoder(self, epoch):\n",
        "        mean_final=0\n",
        "        def get_invalid_indices(vecteur):\n",
        "            indice_1 = np.argwhere(vecteur > 19.).flatten()  # index of invalid values (no reponse after invocation)\n",
        "            indice_2 = np.argwhere(vecteur <= 0.).flatten()  # index of invalid values (none invocation)\n",
        "            indice_invalid_values_ds1 = np.append(indice_1, indice_2) # concatenate of all invalid valeus\n",
        "            return indice_invalid_values_ds1\n",
        "        rtMatrix = pd.read_csv('./rtMatrix.txt', delimiter=\"\\t\", header=None)\n",
        "        rtMatrix = rtMatrix.drop(5825, axis=1)\n",
        "        \n",
        "        lay1= self.generator.layers[0].get_weights()[0].T\n",
        "        lay3= self.generator.layers[3].get_weights()[0].T\n",
        "        lay6= self.generator.layers[6].get_weights()[0].T\n",
        "        lay9= self.generator.layers[9].get_weights()[0].T\n",
        "        \n",
        "        ba1= self.generator.layers[0].get_weights()[1]\n",
        "        ba3= self.generator.layers[3].get_weights()[1]\n",
        "        ba6= self.generator.layers[6].get_weights()[1]\n",
        "        ba9= self.generator.layers[9].get_weights()[1]        \n",
        "        \n",
        "        autoencoder = Sequential()\n",
        "        autoencoder.add(Dense(1024, input_shape=(5825,)))\n",
        "        autoencoder.add(LeakyReLU(alpha=0.2))\n",
        "        autoencoder.add(BatchNormalization(momentum=0.8))\n",
        "        autoencoder.add(Dense(512))\n",
        "        autoencoder.add(LeakyReLU(alpha=0.2))\n",
        "        autoencoder.add(BatchNormalization(momentum=0.8))\n",
        "        autoencoder.add(Dense(256))\n",
        "        autoencoder.add(LeakyReLU(alpha=0.2))\n",
        "        autoencoder.add(BatchNormalization(momentum=0.8))\n",
        "        autoencoder.add(Dense(100,    activation='sigmoid', name=\"bottleneck\"))\n",
        "        \n",
        "        K = tf.keras.backend\n",
        "        K.set_value(autoencoder.layers[0].weights[0], lay9)\n",
        "        K.set_value(autoencoder.layers[3].weights[0], lay6)\n",
        "        K.set_value(autoencoder.layers[6].weights[0], lay3)\n",
        "        K.set_value(autoencoder.layers[9].weights[0], lay1)\n",
        "\n",
        "        K.set_value(autoencoder.layers[0].weights[1], ba6)\n",
        "        K.set_value(autoencoder.layers[3].weights[1], ba3)\n",
        "        K.set_value(autoencoder.layers[6].weights[1], ba1)\n",
        "        #K.set_value(autoencoder.layers[9].weights[1], ba1)\n",
        "\n",
        "        #339 users\n",
        "\n",
        "        for i in range(339):\n",
        "            indice_invalid_values = get_invalid_indices(rtMatrix.iloc[i,:].to_numpy())\n",
        "            vecteur_user_ds1 = np.delete(rtMatrix.iloc[i,:].to_numpy(), indice_invalid_values, axis=None)\n",
        "            vecteur_user_ds1 = np.expand_dims(vecteur_user_ds1, axis=0)\n",
        "            rtMatrixt = np.expand_dims(rtMatrix.iloc[i], axis=0)\n",
        "\n",
        "            train_x = rtMatrixt / 20\n",
        "\n",
        "            encoded_data = autoencoder.predict(train_x)\n",
        "\n",
        "            gen_qos = self.generator.predict(encoded_data)\n",
        "\n",
        "            gen_qos = 20 * gen_qos\n",
        "            reduire = np.delete(gen_qos, indice_invalid_values, axis=None)\n",
        "            reduire= np.expand_dims(reduire, axis=0)\n",
        "\n",
        "            mse = self.root_mean_squared_error(vecteur_user_ds1, reduire)\n",
        "            mse = mse.numpy()\n",
        "\n",
        "            mean_final = mean_final + mse\n",
        "        result=mean_final/339\n",
        "        print(epoch,' rmse with using the autoencoders ',result,' \\n')\n",
        "        return result\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    gan = GAN()\n",
        "    gan.train(epochs=20000, batch_size=273, sample_interval=200)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}